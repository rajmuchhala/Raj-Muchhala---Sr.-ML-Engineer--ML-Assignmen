{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "idfy_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOBZBQb3dFJk8OoxQwODGYF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajmuchhala/idfy/blob/master/idfy_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-3xpFR1xVPK",
        "colab_type": "code",
        "outputId": "e8f23294-2f1b-4430-8ed2-ce00a31f1665",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Importing required modules \n",
        "from zipfile import ZipFile \n",
        "import pandas as pd\n",
        "import string\n",
        "from google.colab import files\n",
        "import cv2\n",
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.layers import Input\n",
        "import keras.backend as K\n",
        "from keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6FmST5FxDRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Upload data on google colab\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-YmbV8NxLAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# specifying the zip file name \n",
        "file_name = \"crnn_model (1).zip\"\n",
        "  \n",
        "# opening the zip file in READ mode \n",
        "with ZipFile(file_name, 'r') as zip: \n",
        "    # printing all the contents of the zip file \n",
        "    zip.printdir() \n",
        "  \n",
        "    # extracting all the files \n",
        "    print('Extracting all the files now...') \n",
        "    zip.extractall() \n",
        "    print('Done!') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzV7KpCcxNOb",
        "colab_type": "code",
        "outputId": "0cc93292-1f68-4f65-93e0-bdccd94e66fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data = pd.read_csv(\"trainVal.csv\") \n",
        "# Preview the first 5 lines of the loaded data \n",
        "data.head()"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>track_id</th>\n",
              "      <th>image_path</th>\n",
              "      <th>lp</th>\n",
              "      <th>train</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>./crop_m1/I00000.png</td>\n",
              "      <td>./crop_m1/I00000.png</td>\n",
              "      <td>9B52145</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>./crop_m1/I00000.png</td>\n",
              "      <td>./crop_h1/I00000.png</td>\n",
              "      <td>9B52145</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>./crop_m1/I00001.png</td>\n",
              "      <td>./crop_m1/I00001.png</td>\n",
              "      <td>6B94558</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>./crop_m1/I00001.png</td>\n",
              "      <td>./crop_h1/I00001.png</td>\n",
              "      <td>6B94558</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>./crop_m1/I00002.png</td>\n",
              "      <td>./crop_m1/I00002.png</td>\n",
              "      <td>8B90164</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               track_id            image_path       lp  train\n",
              "0  ./crop_m1/I00000.png  ./crop_m1/I00000.png  9B52145      0\n",
              "1  ./crop_m1/I00000.png  ./crop_h1/I00000.png  9B52145      0\n",
              "2  ./crop_m1/I00001.png  ./crop_m1/I00001.png  6B94558      0\n",
              "3  ./crop_m1/I00001.png  ./crop_h1/I00001.png  6B94558      0\n",
              "4  ./crop_m1/I00002.png  ./crop_m1/I00002.png  8B90164      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku8577721qPn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Randomly Shuffle the dataframe before splitting the data\n",
        "data = shuffle(data)\n",
        "data.reset_index(inplace=True, drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfvZCdj_xmOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Helper functions to preprocess output labels to indices and vice versa\n",
        "\n",
        "# char_list:   'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n",
        "# total number of our output classes: len(char_list)\n",
        "char_list = string.ascii_letters+string.digits\n",
        "def encode_to_labels(txt):\n",
        "    # encoding each output word into digits\n",
        "    dig_lst = []\n",
        "    for index, char in enumerate(txt):\n",
        "        try:\n",
        "            dig_lst.append(char_list.index(char))\n",
        "        except:\n",
        "            print(char)\n",
        "        \n",
        "    return dig_lst\n",
        "\n",
        "# Reverse translation of numerical classes back to characters\n",
        "def labels_to_text(labels):\n",
        "    ret = []\n",
        "    for c in labels:\n",
        "      if(int(c)!=-1):\n",
        "        ret.append(char_list[c])\n",
        "    return \"\".join(ret)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhG8DRG9x_MY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##  Preprocess Training and Validation data and split them with 80/20 ratio\n",
        "# lists for training dataset\n",
        "training_img = []\n",
        "training_txt = []\n",
        "train_input_length = []\n",
        "train_label_length = []\n",
        "orig_txt = []\n",
        "\n",
        "#lists for validation dataset\n",
        "valid_img = []\n",
        "valid_txt = []\n",
        "valid_input_length = []\n",
        "valid_label_length = []\n",
        "valid_orig_txt = []\n",
        "\n",
        "max_label_len = 0\n",
        "\n",
        "\n",
        "for q in range(len(data)):\n",
        "    # read input image and convert into gray scale image\n",
        "    img = cv2.cvtColor(cv2.imread(data['image_path'][q]), cv2.COLOR_BGR2GRAY) \n",
        "    img = cv2.resize(img, (128, 32))\n",
        "    img = np.expand_dims(img , axis = 2)\n",
        "    # Normalize each image\n",
        "    img = img/255.\n",
        "    \n",
        "    # get the text from the image\n",
        "    txt = data['lp'][q]\n",
        "    \n",
        "    # compute maximum length of the text\n",
        "    if len(txt) > max_label_len:\n",
        "        max_label_len = len(txt)\n",
        "        \n",
        "        \n",
        "    # split the 652 data into validation and training dataset as 20% and 80% respectively\n",
        "    ## (Val = 0.2 * len(data) = 131, Train = len(data) - Val)\n",
        "    if q<131:     \n",
        "        valid_orig_txt.append(txt)   \n",
        "        valid_label_length.append(len(txt))\n",
        "        valid_input_length.append(31)\n",
        "        valid_img.append(img)\n",
        "        valid_txt.append(encode_to_labels(txt))\n",
        "    else:\n",
        "        orig_txt.append(txt)   \n",
        "        train_label_length.append(len(txt))\n",
        "        train_input_length.append(31)\n",
        "        training_img.append(img)\n",
        "        training_txt.append(encode_to_labels(txt)) \n",
        "    \n",
        "    \n",
        "# pad each output label to maximum text length\n",
        "\n",
        "train_padded_txt = pad_sequences(training_txt, maxlen=max_label_len, padding='post', value = len(char_list))\n",
        "valid_padded_txt = pad_sequences(valid_txt, maxlen=max_label_len, padding='post', value = len(char_list))\n",
        "\n",
        "### Covert the lists to numpy arrays\n",
        "training_img = np.array(training_img)\n",
        "train_input_length = np.array(train_input_length)\n",
        "train_label_length = np.array(train_label_length)\n",
        "\n",
        "valid_img = np.array(valid_img)\n",
        "valid_input_length = np.array(valid_input_length)\n",
        "valid_label_length = np.array(valid_label_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN1YKdlEyw1_",
        "colab_type": "code",
        "outputId": "13455daa-505a-4956-c39e-43d3cc5090ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        }
      },
      "source": [
        "#### Define the model \n",
        "\n",
        "##Input layer with dimensions(None,32,128,1)\n",
        "inputs = layers.Input(shape=(32,128,1))\n",
        " \n",
        "# convolution layer with kernel size (3,3)\n",
        "conv_1 = layers.Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n",
        "# poolig layer with kernel size (2,2)\n",
        "pool_1 = layers.MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
        " \n",
        "conv_2 = layers.Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
        "pool_2 = layers.MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
        " \n",
        "conv_3 = layers.Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
        " \n",
        "conv_4 = layers.Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
        "# poolig layer with kernel size (2,1)\n",
        "pool_4 = layers.MaxPool2D(pool_size=(2, 1))(conv_4)\n",
        " \n",
        "conv_5 = layers.Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n",
        "# Batch normalization layer\n",
        "batch_norm_5 = layers.BatchNormalization()(conv_5)\n",
        " \n",
        "conv_6 = layers.Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
        "batch_norm_6 = layers.BatchNormalization()(conv_6)\n",
        "pool_6 = layers.MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
        " \n",
        "conv_7 = layers.Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
        " \n",
        "squeezed = layers.Lambda(lambda x: tf.keras.backend.squeeze(x, 1))(conv_7)\n",
        " \n",
        "# bidirectional LSTM layers with units=128\n",
        "lstm_1 = layers.LSTM(128, return_sequences=True, dropout = 0.2)\n",
        "lstm_2 = layers.LSTM(128, return_sequences=True, dropout = 0.2)\n",
        "blstm_1 = layers.Bidirectional(lstm_1)(squeezed)\n",
        "blstm_2 = layers.Bidirectional(lstm_2)(blstm_1)\n",
        " \n",
        "outputs = layers.Dense(len(char_list)+1, activation = 'softmax')(blstm_2)\n",
        "\n",
        "# model to be used at test time\n",
        "act_model = Model(inputs, outputs)\n",
        "\n",
        "act_model.summary()"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_33\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_17 (InputLayer)        [(None, 32, 128, 1)]      0         \n",
            "_________________________________________________________________\n",
            "conv2d_112 (Conv2D)          (None, 32, 128, 64)       640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_64 (MaxPooling (None, 16, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_113 (Conv2D)          (None, 16, 64, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_65 (MaxPooling (None, 8, 32, 128)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_114 (Conv2D)          (None, 8, 32, 256)        295168    \n",
            "_________________________________________________________________\n",
            "conv2d_115 (Conv2D)          (None, 8, 32, 256)        590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_66 (MaxPooling (None, 4, 32, 256)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_116 (Conv2D)          (None, 4, 32, 512)        1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_32 (Batc (None, 4, 32, 512)        2048      \n",
            "_________________________________________________________________\n",
            "conv2d_117 (Conv2D)          (None, 4, 32, 512)        2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 4, 32, 512)        2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_67 (MaxPooling (None, 2, 32, 512)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_118 (Conv2D)          (None, 1, 31, 512)        1049088   \n",
            "_________________________________________________________________\n",
            "lambda_16 (Lambda)           (None, 31, 512)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_32 (Bidirectio (None, 31, 256)           656384    \n",
            "_________________________________________________________________\n",
            "bidirectional_33 (Bidirectio (None, 31, 256)           394240    \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 31, 63)            16191     \n",
            "=================================================================\n",
            "Total params: 6,619,711\n",
            "Trainable params: 6,617,663\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSr5Lc6CzZIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Define CTC loss function and the training model using CTC loss\n",
        "\n",
        "labels = layers.Input(name='the_labels', shape=[max_label_len], dtype='float32')\n",
        "input_length = layers.Input(name='input_length', shape=[1], dtype='int64')\n",
        "label_length = layers.Input(name='label_length', shape=[1], dtype='int64')\n",
        " \n",
        "def ctc_lambda_func(args):\n",
        "    y_pred, labels, input_length, label_length = args\n",
        " \n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
        " \n",
        " \n",
        "loss_out = layers.Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, labels, input_length, label_length])\n",
        "\n",
        "#model to be used at training time\n",
        "model = Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7pNb8H0zpuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Compile the model\n",
        "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = 'adam',metrics=['accuracy'])\n",
        "\n",
        "##Load pre-trained model as new_model\n",
        "pretrained_model = load_model(\"crnn_model.h5\")\n",
        "\n",
        "##Load pre-trained weights and use them as initialization to re-train the model with our own data\n",
        "act_model.set_weights(pretrained_model.get_weights())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsRdnXaI0Eof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Save the model after fine tuning on our own dataset\n",
        "filepath=\"best_model.hdf5\"\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opuNEXoV0GhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Train the model\n",
        "batch_size = 16\n",
        "epochs = 10\n",
        "history = model.fit(x=[training_img, train_padded_txt, train_input_length, train_label_length], y=np.zeros(len(training_img)), batch_size=batch_size, epochs = epochs, validation_data = ([valid_img, valid_padded_txt, valid_input_length, valid_label_length], [np.zeros(len(valid_img))]), verbose = 1, callbacks = callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmTtjyKa1XYB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "ca1c482b-1650-4310-ce50-ad9abd050276"
      },
      "source": [
        "### Plot the training and validation loss versus epochs\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXgkd33n8fe3W637HI3mlOQZe2zPjC/JFmCwCQQcAjY+dokxBPtJQp6YbMzaJpBglmRDNsmG3bBJOEzAxA6EOOawDXbAYGKwIY6NYS58zIzxNYfm1Gh0Xy11f/ePKkktzWFJ062Suj+v59HTreo6vmp7Pr+qX1X9ytwdEREpHLGoCxARkfml4BcRKTAKfhGRAqPgFxEpMAp+EZECo+AXESkwCn6RkzCzL5vZX85w3l1mdtmprkck1xT8IiIFRsEvIlJgFPyy6IVdLH9kZk+b2YCZ3Wlmy83se2bWZ2aPmFldxvxXmdlzZtZtZo+Z2YaMz1rNbEu43NeB0mnbeqeZbQuXfcLMzp9jzb9nZi+a2VEze9DMVoXTzcz+zswOm1mvmT1jZueGn11uZtvD2vaZ2Ufm9IVJwVPwS754F/BrwFnAlcD3gP8BNBD8f34zgJmdBdwD3Bp+9hDwb2ZWbGbFwLeBrwJLgG+G6yVcthW4C/gAUA98EXjQzEpmU6iZvQX4a+DdwEpgN/C18OO3Ab8S/h014Tyd4Wd3Ah9w9yrgXOBHs9muyDgFv+SLz7r7IXffB/wH8JS7b3X3YeBbQGs433XAd9393919FPgUUAa8AbgYSAB/7+6j7n4v8POMbdwIfNHdn3L3lLt/BRgJl5uN9wF3ufsWdx8BPga83szWAKNAFbAeMHff4e4HwuVGgY1mVu3uXe6+ZZbbFQEU/JI/DmW8HzrO75Xh+1UEe9gAuHsa2AusDj/b51NHLtyd8f404MNhN0+3mXUDTeFyszG9hn6CvfrV7v4j4HPA7cBhM7vDzKrDWd8FXA7sNrMfm9nrZ7ldEUDBL4VnP0GAA0GfOkF47wMOAKvDaeOaM97vBf7K3Wszfsrd/Z5TrKGCoOtoH4C7f8bdLwI2EnT5/FE4/efufjWwjKBL6huz3K4IoOCXwvMN4Aoze6uZJYAPE3TXPAE8CYwBN5tZwsz+K/DajGW/BPy+mb0uPAlbYWZXmFnVLGu4B/gdM2sJzw/8b4KuqV1m9ppw/QlgABgG0uE5iPeZWU3YRdULpE/he5ACpuCXguLuzwPXA58FjhCcCL7S3ZPungT+K/DbwFGC8wH3Zyy7Cfg9gq6YLuDFcN7Z1vAI8KfAfQRHGWcA7wk/riZoYLoIuoM6gb8JP7sB2GVmvcDvE5wrEJk104NYREQKi/b4RUQKjIJfRKTAKPhFRAqMgl9EpMAURV3ATCxdutTXrFkTdRkiIovK5s2bj7h7w/TpiyL416xZw6ZNm6IuQ0RkUTGz3cebnrOuHjO7Kxxh8NmMaUvM7N/N7IXwte5k6xARkezLZR//l4G3T5t2G/BDdz8T+GH4u4iIzKOcBb+7/4Tg7sdMVwNfCd9/BbgmV9sXEZHjm+8+/uUZQ8weBJafaEYzu5FgGFyam5uP+Xx0dJT29naGh4dzUeeCUVpaSmNjI4lEIupSRCRPRHZy193dzE44XoS73wHcAdDW1nbMfO3t7VRVVbFmzRqmDqaYP9ydzs5O2tvbWbt2bdTliEiemO/r+A+Z2UqA8PXwXFc0PDxMfX193oY+gJlRX1+f90c1IjK/5jv4HwR+K3z/W8ADp7KyfA79cYXwN4rI/Mrl5Zz3EIxvfraZtZvZ7wKfBH7NzF4ALgt/z5nuwSSd/SO53ISIyKKTy6t63uvuK9094e6N7n6nu3e6+1vd/Ux3v8zdp1/1k1U9Q6N05Cj4u7u7+fznPz/r5S6//HK6u7tzUJGIyMzk9Vg95cVxkmNpRlPZf1DRiYJ/bGzspMs99NBD1NbWZr0eEZGZWhRDNsxVeXHw5w0lUyTKstvG3Xbbbbz00ku0tLSQSCQoLS2lrq6OnTt38stf/pJrrrmGvXv3Mjw8zC233MKNN94ITA4/0d/fzzve8Q4uvfRSnnjiCVavXs0DDzxAWVlZVusUEZkuL4L/z//tObbv7z3uZwMjYySKYhTHZxf8G1dV82dXnnPCzz/5yU/y7LPPsm3bNh577DGuuOIKnn322YnLLu+66y6WLFnC0NAQr3nNa3jXu95FfX39lHW88MIL3HPPPXzpS1/i3e9+N/fddx/XX3/9rOoUEZmtvAj+k4nFjHTaIZ7b7bz2ta+dcq39Zz7zGb71rW8BsHfvXl544YVjgn/t2rW0tLQAcNFFF7Fr167cFikiQp4E/8n2zPd1DdI9OMrGVdU5vTSyoqJi4v1jjz3GI488wpNPPkl5eTlvfvObj3stfklJycT7eDzO0NBQzuoTERmX1yd3IejnT7kzMpbdE7xVVVX09fUd97Oenh7q6uooLy9n586d/PSnP83qtkVETkVe7PGfTFlx0MczmExRmshef099fT2XXHIJ5557LmVlZSxfPjns0Nvf/na+8IUvsGHDBs4++2wuvvjirG1XRORUmfsJh8tZMNra2nz6g1h27NjBhg0bXnVZd2f7gV5qyhI01pXnqsScmunfKiKSycw2u3vb9Ol539VjZpQXFzGYTEVdiojIgpD3wQ/BjVwjoylS6YV/dCMikmsFEfxlxXEcGEqe/K5aEZFCUBDBXx6e1B0cVXePiEhBBH9RPEZJUZzBEQW/iEhBBD8E/fyDoykWw1VMIiK5VFDBP5bKzUidM1FZWRnJdkVEpiuY4M+8kUtEpJDl/Z2740oTcWJmDCZT1GbhPq7bbruNpqYmbrrpJgA+8YlPUFRUxKOPPkpXVxejo6P85V/+JVdfffWpb0xEJIvyI/i/dxscfOaks8SAM8av6pnJ0A0rzoN3nPjJkNdddx233nrrRPB/4xvf4OGHH+bmm2+murqaI0eOcPHFF3PVVVfpubkisqDkR/DPUMxgNO04jnFqYdza2srhw4fZv38/HR0d1NXVsWLFCj70oQ/xk5/8hFgsxr59+zh06BArVqzI0l8gInLq8iP4T7JnnmloMMmeo4OsW1Y58XSuU3Httddy7733cvDgQa677jruvvtuOjo62Lx5M4lEgjVr1hx3OGYRkSgVzMldmHwUY7ZO8F533XV87Wtf49577+Xaa6+lp6eHZcuWkUgkePTRR9m9e3dWtiMikk35scc/Q4m4kYjHshb855xzDn19faxevZqVK1fyvve9jyuvvJLzzjuPtrY21q9fn5XtiIhkU0EFfzBSZzyrY/Y888zkSeWlS5fy5JNPHne+/v7+rG1TRORUFFRXDwTX84+MpRmL6EYuEZGoFVzwZ7ufX0RksVnUwT+XcXfKEnGMxRP8GltIRLJt0QZ/aWkpnZ2dsw7GeMwoScQZXARj87s7nZ2dlJaWRl2KiOSRRXtyt7Gxkfb2djo6Oma9bNdgkqFkiuGOMhb6TbWlpaU0NjZGXYaI5JFFG/yJRIK1a9fOadlvbNrLHz/wNI/84ZtYt0yjZopIYVm0XT2n4sLmWgC27umKuBIRkflXkMF/+tJKqkqL2Lq3O+pSRETmXUEGfyxmtDTVsm2Pgl9ECk9BBj9Aa1MtOw/2Loqre0REsimS4DezD5nZc2b2rJndY2bzfr1iS3MtaYdn2nvme9MiIpGa9+A3s9XAzUCbu58LxIH3zHcdLU11AOrnF5GCE1VXTxFQZmZFQDmwf74LWFJRzGn15ernF5GCM+/B7+77gE8Be4ADQI+7/2D6fGZ2o5ltMrNNc7lJayZam2rZuleXdIpIYYmiq6cOuBpYC6wCKszs+unzufsd7t7m7m0NDQ05qaWlqZZDvSMc6BnKyfpFRBaiKLp6LgNecfcOdx8F7gfeEEEdtDaH/fzq7hGRAhJF8O8BLjazcjMz4K3AjgjqYMPKaoqLYmzTCV4RKSBR9PE/BdwLbAGeCWu4Y77rACguinHuqmoN3SAiBSWSq3rc/c/cfb27n+vuN7j7SBR1QHBZ59PtPYzqiVwiUiAK9s7dca3NtYyMpXn+YF/UpYiIzAsFv0bqFJECU/DBv7q2jKWVJbqDV0QKRsEHv5nR2qyROkWkcBR88ENwI9fLRwboHkxGXYqISM4p+Jns59f1/CJSCBT8wPmNtZgp+EWkMCj4gcqSIs5eXqWhG0SkICj4Qy1NtWzb2427R12KiEhOKfhDrc219AyN8sqRgahLERHJKQV/aOKJXOruEZE8p+APrVtWSWVJkU7wikjeU/CH4jHjgqYaPZFLRPKegj9DS1MtOw/0MZRMRV2KiEjOKPgztDbVMZZ2nt3fE3UpIiI5o+DP0DJ+B69O8IpIHlPwZ1haWULTkjL184tIXlPwT9PSVKc9fhHJawr+aVqbatnfM8zBnuGoSxERyQkF/zQT/fzq7hGRPKXgn+acVdUUx2N6IpeI5C0F/zQlRXE2rKrW0A0ikrcU/MfR2lTLM+09jKXSUZciIpJ1Cv7jaG2uZWg0xfOH+qIuRUQk6xT8x9EajtSpAdtEJB8p+I+jaUkZ9RXF6ucXkbyk4D8OM5t4IpeISL5R8J9Aa3MtLx7up2doNOpSRESySsF/AuNP5Hq6XXv9IpJfFPwncH5TDWZ6FKOI5B8F/wlUlyZY11DJ1j0aukFE8ouC/yRam4MTvO4edSkiIlkTSfCbWa2Z3WtmO81sh5m9Poo6Xk1LUx1dg6Ps7hyMuhQRkayJao//08D33X09cAGwI6I6Tqp1YqRO9fOLSP6Y9+A3sxrgV4A7Adw96e4LMlnPWl5FeXFc/fwiklei2ONfC3QA/2RmW83sH82sYvpMZnajmW0ys00dHR3zXyUQjxnnN9Zoj19E8koUwV8EXAj8g7u3AgPAbdNncvc73L3N3dsaGhrmu8YJrc11bD/Qy/BoKrIaRESyKYrgbwfa3f2p8Pd7CRqCBamlqZbRlPPc/t6oSxERyYp5D353PwjsNbOzw0lvBbbPdx0z1doUnOBVP7+I5IuiiLb734G7zawYeBn4nYjqeFXLqktZXVumRzGKSN6IJPjdfRvQFsW256KluZZtGrpBRPKE7tydgdamWvZ1D3G4bzjqUkRETpmCfwYmbuTSXr+I5AEF/wycs6qGopipn19E8oKCfwZKE3E2rqrWHr+I5AUF/wy1NtXydHs3qbRG6hSRxU3BP0MtzbUMJFO8cLgv6lJERE6Jgn+GWsNHMeqJXCKy2Cn4Z+i0+nLqyhPq5xeRRU/BP0NmRktTLVv3augGEVncZhT8ZnaLmVVb4E4z22Jmb8t1cQtNS1MdLxzup294NOpSRETmbKZ7/O93917gbUAdcAPwyZxVtUC1NtfiDk+390RdiojInM00+C18vRz4qrs/lzGtYFygkTpFJA/MNPg3m9kPCIL/YTOrAtK5K2thqilLcEZDhZ7IJSKL2kxH5/xdoAV42d0HzWwJC3go5VxqaarjsecP4+6YFdxBj4jkgZnu8b8eeN7du83seuBPgILs6G5trqVzIEl711DUpYiIzMlMg/8fgEEzuwD4MPAS8M85q2oBGx+pc4v6+UVkkZpp8I+5uwNXA59z99uBqtyVtXCdvbyKskRc/fwismjNtI+/z8w+RnAZ5xvNLAYkclfWwlUUj3FeY42GbhCRRWume/zXASME1/MfBBqBv8lZVQtca1Mt2/f3MjKWiroUEZFZm1Hwh2F/N1BjZu8Eht29IPv4IejnT6bSbN/fG3UpIiKzNtMhG94N/Ay4Fng38JSZ/UYuC1vIWjRSp4gsYjPt4/848Bp3PwxgZg3AI8C9uSpsIVtRU8rKmlKd4BWRRWmmffyx8dAPdc5i2bykkTpFZLGaaXh/38weNrPfNrPfBr4LPJS7sha+1uZa9h4d4kj/SNSliIjMykxP7v4RcAdwfvhzh7t/NJeFLXTj/fx6MIuILDYz7ePH3e8D7sthLYvKeatriMeMbXu7uWzj8qjLERGZsZMGv5n1AX68jwB39+qcVLUIlBXH2bCySv38IrLonDT43b0gh2WYqZamWr69dT+ptBOPaaROEVkcCvrKnFPV2lRH/8gYL3X0R12KiMiMKfhPQUuznsglIouPgv8UrK2voKYsoRu5RGRRUfCfgljMuKCpVkM3iMiiouA/Ra1NtfzyUB/9I2NRlyIiMiORBb+Zxc1sq5l9J6oasqGluZa0w9Pt2usXkcUhyj3+W4AdOd3Cvi2w87s53URLY3CCV/38IrJYRBL8ZtYIXAH8Y0439O//E77136D3QM42UVdRzOlLK9TPLyKLRlR7/H8P/DGQPtEMZnajmW0ys00dHR1z28qVn4bUCHznQ+DHuwE5O1qaatm2txvP4TZERLJl3oM/fILXYXfffLL53P0Od29z97aGhoa5baz+DHjLn8AvvwfP5m6YodbmWjr6RtjXPZSzbYiIZEsUe/yXAFeZ2S7ga8BbzOxfcra1i/8AVrfBQ38E/Ydfff45mBipU/38IrIIzHvwu/vH3L3R3dcA7wF+5O7X52yDsThcfTsk+4Pwz4H1K6soKYqpn19EFoXCuI5/2Xp400dh+7dh+wNZX30iHuO81TUaukFEFoVIg9/dH3P3d87Lxi65BVZeAN/9CAwezfrqW5treXZ/L8mxE56vFhFZEApjjx8gngi6fIaOwvdvy/rqW5rqSI6l2XGgN+vrFhHJpsIJfoAV58EbPwxPfx2e/35WV93arBu5RGRxKKzgB3jjR2DZRvjOrTCUvZBeWVPKsqoS9fOLyIJXeMFfVBx0+fQfgh/8SdZWa2a0Ntdqj19EFrzCC36A1RfCG26GrV+Fl36UtdW2Ntexq3OQowPJrK1TRCTbCjP4Ad58G9SfCQ/eAiN9WVllS1PQz/8L7fWLyAJWuMGfKAu6fHr2wiN/npVVnt9YQ8z0KEYRWdgKN/gBml8HF/83+PmXYNfjp7y68uIizl5RzVbt8YvIAlbYwQ/BIG51a+CBD0Jy8JRXN36CN53WSJ0isjAp+Isr4KrPQdcr8OhfnfLqWppq6Rse4+Uj/VkoTkQk+xT8AGvfCG2/C0/eDnt/dkqrujC8kUsDtonIQqXgH/drfw41jfDATTA6POfVnL60kqrSIvXzi8iCpeAfV1IFV/49HPkl/Pj/zHk1sZgFT+TSHr+ILFAK/kzrLoOW6+E/Pw37t855NS1Ntew82MtgciyLxYmIZIeCf7pf/yuoaIBv3wRjc7sDt7W5lrTDM+09WS5OROTUKfinK6uFd/4dHH4OHv/bOa1i/FGM6ucXkYVIwX886y+H866Fn/wNHHx21osvqSjmtPpy9fOLyIKk4D+Rd/xfKKsLrvJJzb6vvrWpli17unDXjVwisrAo+E+kfAlc/ik4sA2e+MysF29pquVw3wgHeuZ+aaiISC4o+E/mnGtgw1Xw2Ceh4/lZLdraHPTza3x+EVloFPyv5or/B8XlwVg+6dSMF9uwspriophG6hSRBUfB/2oqlwX9/e0/g6e+MOPFiotinLuqWnv8IrLgKPhn4rxr4ay3ww//AjpfmvFiLU11PN3ew2gqncPiRERmR8E/E2bBtf3xBDx4M6RnFuStzbWMjKV5/mB2nvAlIpINCv6Zql4V3NW7+3HYfNeMFhl/FKP6+UVkIVHwz0brDXD6r8K//xl073nV2RvrylhaWaI7eEVkQVHwz4YZXBVe0//gzfAqN2eZWfBELt3BKyILiIJ/tmqb4bJPwMuPwtZ/edXZW5pqefnIAN2DcxvwTUQk2xT8c9H2u3DapfDwx6F3/0lnbQ2fyKXLOkVkoVDwz0UsFnT5pJLwnQ+dtMvn/MZazOCx5zvmsUARkRNT8M9V/Rnw1j+FX34fnvnmCWerLCni7ees4MtP7OIj3/yFHs4iIpFT8J+K1/0+NL4WvvfH0H/4hLN99r2t3PyWddy3pZ2rP/ef/PKQrusXkejMe/CbWZOZPWpm283sOTO7Zb5ryJpYHK6+HZKD8N0Pn3C2oniMP3zb2Xz1/a+ja3CUqz73OF//+R4N2SwikYhij38M+LC7bwQuBm4ys40R1JEdDWfBm2+DHQ/Cc98+6ayXnrmUh265lItOq+Oj9z3DrV/fRv+Iun5EZH7Ne/C7+wF33xK+7wN2AKvnu46sesPNsLIFHvoIDHSedNZlVaX88/tfx0fedhb/9ov9XPnZx3luv57NKyLzJ9I+fjNbA7QCTx3nsxvNbJOZberoWOBXxMSL4JrPw1A3fP+jrz57zPjgW87knt+7mMHkGP/l80/w1Sd3qetHROZFZMFvZpXAfcCt7t47/XN3v8Pd29y9raGhYf4LnK3l58CvfCS4wmfnQzNa5HWn1/PQzW/kDWfU86cPPMdN/7qF3uHRHBcqIoUukuA3swRB6N/t7vdHUUNOXPqHsOyc4Nr+oZndsFVfWcJdv/UaPvaO9Tz83CGu+Mx/8Avd7CUiORTFVT0G3AnscPe/ne/t51RRMVxzOwx0wA8+PuPFYjHjA286g2984PWk0/AbX3iCOx9/RV0/IpITUezxXwLcALzFzLaFP5dHUEdurGqFS24JxvF58ZFZLXrRaXV89+ZLedNZy/iL72zn9/55s8b4EZGss8WwV9nW1uabNm2KuoyZGx2GL74xuL7/D56E0upZLe7u/NN/7uKvv7eDZVWlfOa9rVx0Wl2OihWRfGVmm929bfp03bmbC4nS4Mau3n3wyCdmvbiZ8f5L13Lv77+BWAze/cUn+cKPXyKdXviNtIgsfAr+XGl6Lbz+Jth0J7zyH3NaxQVNtXz35jfy6+cs55Pf28n7v/JzOvtHslyoiBQaBX8u/erHoW4tPPhBSA7MaRXVpQlu/80L+YtrzuWJlzq5/DP/wVMvn/wmMRGRk1Eff67tehy+fAVUrYSlZ0Ldmsmf2vC1fEnwdK9X8dz+Hj74r1vZ3TnAhy47iz/41XXEY6++nIgUphP18Sv458PT34QXHoau3dC1CwamjeRZXBU2BqdNbRjq1gRP/CoqmZi1f2SMj3/rGR7Ytp9L1tXzd9e1sKyqdN7+FBFZPBT8C0lyYLIR6NoF3Rnvu3bB2HDGzAbVq8JGIGgYvO40fniwnE88PsBw8VI+/d5WLlm3NII/REQWMgX/YuEO/YfCRmBag9C1C/qmPupxmGL2pBuI169l7ZnnEFuyNuNo4TQoLp/vv0BEFogTBX9RFMXISZhB1Yrgp/niYz8fHYaevRMNQfzIyyR3PEO8czcj3Zsp86Gp81csm9p1VH8G1K+DJacH5xZEpOAo+BebRGlwknjpmcGvwLmXw/1b2nnXt59hRdEg//eyatqqeqYeKez9KTx7L3h6cl1lS4JGoH7dZIMw3ijoSEEkb6mrJ4+8eLifD/7rFnYe7OMDbzqdj7ztbBLxjCt2x5LB+YTOF8OflyZfp3UhUd2Y0RhkNAq1zRBPzO8fJiJzoj7+AjE8muJ/fWc7//rUHi5sruWzv3khq2vLXn3BkX44+vK0BuFF6HwBhjMeFBMrCruMjnOkULVyRpelisj8UPAXmH/7xX4+dv8zxGPGp669gF/buHxuK3KHwaNwNLMxGG8cXoKxjHMKiXJYcsbUxmC8cdD5hEkDR+DwDujYCYe3w0gfVK+G2iaoaQ5fm6CkMupKZZFT8BegXUcG+OA9W3h2Xy/vv2Qtt71jPcVFWbxZO50OuoimNwadLwbnFTw1Oe+U8wmnB3c0162FJWuhrC4/jxSGuuDwTujYEQT9eNgPZDxRrrQm+OndD+lpz18uqwsagJqmycZg4rUZyuvz83uTrFHwF6iRsRR//dBOvvzELs5vrOG337CG9SuqWbesMruNwHSp0eBy1ClHCSc4n1BSPe3mtYxLUmuaguccLGTDvdDxfBjw4V58x07oOzA5T3ElNKyHZeth2cbw/cbg6i0zSKeCy3i79wZXbXXvCV572ienJfunbreoDGoapzUKzZPTqlYFjwWVgqXgL3Dff/Ygt93/NN2DwaMdi2LGumWVrF9RxfqV1WxYWc2GFVU0VJVgud6LTA4EwXb0lWPvU+jaBamMgegsFpxorjstODqYcmfzPB8tJAfhyPOTe+/je/A9eyfnKSqDhrNh2Ybgp2FDEPY1TadWp3twBNGzN2wI2qc2EN17YfDI1GUsHtz8l3mkMNFQhA2Ert7Kawp+YSyV5pUjA+w42MfOA73sPNjHjgO9HOiZvFN4SUUx61dUsWFl9cTrumWVlCbi81NkOg39B49tDMYbienDXZTUHH+oiyVrg6CbyxVIo8PBSe3McD+8PTiCIfz3Ei+GpWcHoT6+975sfXDTXGyevqtj6h4KjxAyGoPMo4befVO73wDKl4YNQWPw96y+KPipmuM5IVlQFPxyQt2DSXaGjcGOA33sPNjL84f6GB4NrvmPx4zTl1awfqIxCBqEFdWluT86mG7KcBfTjxh2H3u0UNN47FHC+PuSqqD7aSLgw66aoy9N3u8QK4L6M8OA3zC5J1+3dvF1o6TGgu6nKY1C+L57T3BV13jDUN0Iqy+cbAhWtQTflywqCn6ZlVTa2d05MHFUMN4gtHdNXsVTU5aYOCrYsLKK9SuqOWt5FWXFEe3xjh8tnKgLafrRAsbEHrzFghvXMrtnlm0MrlJa6OcYsiU5CAefhn2bJ3+6doUfWnBks/qiyQZh+Tm6p2OBU/BLVvQOj/L8+NFBRpfRYDLYUzSDtfUVE11F40cJjXVl8390MN1I/9QB8Ya6gzugl20I9uoTGuX0GAOdsH/L1MZgMHweRFEprDh/8qhg9YVB4xn1f+fZSqeCI6HxI5+ePdB3KDjxvvTM4P+NJacvyv8/FPySM+m0s7drcOKoYOeBPnYc7GV35+DEPFUlRawPjwrOXlHFafXlNNWVs6q2LLdXF0l2uQeN577NsC9sEPZvm7yfo6wOVmV0Ea2+CCoboq15LAm94bmP7mndWz17jn8pbUkNjGTcuIgFl9DWrwsbg3WTjUL1qgXb2Cn4Zd4NjIzx/KE+doYNwo4DQaPQNzL5jyxmsKK6lMYlQUPQtKQsfC2nsa6M5dWletjMQpcaC86PTBwVbIXDz02eJ6lpnhJk7SwAAApGSURBVHq+YOUF2b05LTlw7GWwmSHfd5CJLj1gYqjz8audapuPvRy2uDw4Qsy8FPnIC8FJ/yMvwmjGE/US5eFNi2dONgZLw3tWIj4vouCXBcHdOdAzzJ6jg+w9OsjeriHajw7S3jXE3q5BDvYOk/m/ZCJurK4tCxuCqQ1DU10ZSyqKo+9CkmMlB+DAtPMF3buDzywWnEfJbAyWbTzxyfKh7mOvVOrePfl+cNqjSGMJqFkdhvlpx978Vr361M7buAddQ+MNQedLk++790wdCLFyxbFHCEvXBQ3MPFwcoOCXRWFkLMX+7uGwURhk79GgQWgPG4mjA8kp85cXx2msm3qU0JRx9FBVqpOPC8bAkcnuofGfoaPBZ0VlwZHAqtag2yUz5Ed6p66nqGzqHcyZQ13UNkPl8uguqR0bCa6OOl6jMNQ1OV+8OLgybEqjsC5oGCrqs1aOgl/ywsDI2ESD0J7RMOwNjxr6R6b21daWJ2iqy2wQyia6lVbVllKWiOuIISruwUn2zPMFB7ZBvOQ4XTDjIb+Ih6oY6Ay7iqY1CkdfhvTo5HxldRndRuug9XqoXDanTSr4Je+5O92Do1OOFKZ3JyVT6SnLxGNGRXGcypIiKsKfqtIiKoqD95Ul8YnpleFPxcTr1OUqS4oWzfkId2c05Yym0oym0iRTacZSTiodTEulnbG0M5ZyxtLpcLqH09Ph9Km/TyyT8ftoOk0qlTE97VN+n7peJ5VKUVVaTENVCUsri2moKg1fS2ioKqGypCj/GurU2ORw6dMbhf6DcPO24IbEOdATuCTvmRl1FcXUVRRzfmPtMZ+n005H/8hEN9KBnmEGRsboHx6jfyTFwMgYA8kx+kfGONQ7HE4fYyCZIpWe2Q5SWSJ+TINRNaVxiGc0HMFrIh6bCOAghJ3RsWm/p9IT05IZgT2aSpMc8xMunwyXm77MaGr+d/jiMaMo/InHjEQ8NjktHqMoZsRiRu9QN50DyeN+5yVFsYlGYGll8NpQWcLS8LWhqpiGylIaqkqiu59ktuJF4cnhM+CsX5/62XBvMM5Tlin4pWDEYsby6lKWV5fStmbmw0S7OyNj6aARGBmjb3gso5EIG4wTTO8fGQsamOTYxO/jd0TPRiIeBOX4T3HcKC6a/D1RFExLxGOUF4fzFB27zPi809dRFP4ej8XCIB4P6FjG+2lhHX42PbzjMSMRixGPTw362eypp9NO12CSjv4RjvQl6egfDl9H6Ogb4UjYgG/d00XnQJLjdVxUFMePaSSOaSzCI4uSogXaSJRW52S1Cn6RV2FmlCbilCbiLK0sOeX1jaXSDCRTEw1JcixNSVEsI5SN4ozATsRnF5r5IBYz6itLqK8sgRUnn3cslebowGSjEDQMyYkGoqNvhBcO9/Pky50TgxROV11adEwDsbQyOHqsryimrryY+spillSUUFOWWDRdeiei4BeZZ0XxGDVlMWrKdMVRNhTFYyyrLmVZ9avfWTsylqKzPznRIBw5TmPx3P5ejvSNTLnfJJMZ1JUXU1eeoL6ihLqKBEsqSoIGYryhmPY6b4MczpCCX0QKRklRnFW1ZayaweNIR8ZSdA+O0tmf5OhAkqODSY72j3B0cJSjAyN0DYzSOTDCK0cG2Ly7m67B45+XgOCy4/GjhrryyUZhyQkaiurSBLEcHlUo+EVEjqOkKM7y6jjLZ3AkAcF5ib7hMToHRugaTE5rMMLXgSRdA0le6uinayDJQDJ13HXFY0ZdeYIlFcV88YY21i6tyOafpuAXEcmGWMyoKU9QUz7zLrzh0VTQOIQ/4w1G12CSzoGgwagsyX5MK/hFRCJSmph511M2RTIsopm93cyeN7MXzey2KGoQESlU8x78ZhYHbgfeAWwE3mtmG+e7DhGRQhXFHv9rgRfd/WV3TwJfA66OoA4RkYIURfCvBvZm/N4eTpvCzG40s01mtqmjo2PeihMRyXcL9tFH7n6Hu7e5e1tDQ8RP8BERySNRBP8+oCnj98ZwmoiIzIMogv/nwJlmttbMioH3AA9GUIeISEGa9+v43X3MzD4IPAzEgbvc/bn5rkNEpFAtigexmFkHsHuOiy8FjmSxnMVO38ckfRdT6fuYKh++j9Pc/ZiTpIsi+E+FmW063hNoCpW+j0n6LqbS9zFVPn8fC/aqHhERyQ0Fv4hIgSmE4L8j6gIWGH0fk/RdTKXvY6q8/T7yvo9fRESmKoQ9fhERyaDgFxEpMHkd/Br3P2BmTWb2qJltN7PnzOyWqGtaCMwsbmZbzew7UdcSNTOrNbN7zWynme0ws9dHXVNUzOxD4b+TZ83sHjOb2bMXF5G8DX6N+z/FGPBhd98IXAzcVMDfRaZbgB1RF7FAfBr4vruvBy6gQL8XM1sN3Ay0ufu5BKMLvCfaqrIvb4Mfjfs/wd0PuPuW8H0fwT/qY4bCLiRm1ghcAfxj1LVEzcxqgF8B7gRw96S7d0dbVaSKgDIzKwLKgf0R15N1+Rz8Mxr3v9CY2RqgFXgq2koi9/fAHwPpqAtZANYCHcA/hV1f/2hmFVEXFQV33wd8CtgDHAB63P0H0VaVffkc/DKNmVUC9wG3untv1PVExczeCRx2981R17JAFAEXAv/g7q3AAFCQ58TMrI6gZ2AtsAqoMLPro60q+/I5+DXufwYzSxCE/t3ufn/U9UTsEuAqM9tF0AX4FjP7l2hLilQ70O7u40eB9xI0BIXoMuAVd+9w91HgfuANEdeUdfkc/Br3P2RmRtB/u8Pd/zbqeqLm7h9z90Z3X0Pw/8WP3D3v9upmyt0PAnvN7Oxw0luB7RGWFKU9wMVmVh7+u3kreXiie97H458vGvd/ikuAG4BnzGxbOO1/uPtDEdYkC8t/B+4Od5JeBn4n4noi4e5Pmdm9wBaCq+G2kodDN2jIBhGRApPPXT0iInIcCn4RkQKj4BcRKTAKfhGRAqPgFxEpMAp+kRwzszdrBFBZSBT8IiIFRsEvEjKz683sZ2a2zcy+GI7X329mfxeOz/5DM2sI520xs5+a2dNm9q1wjBfMbJ2ZPWJmvzCzLWZ2Rrj6yozx7u8O7woViYSCXwQwsw3AdcAl7t4CpID3ARXAJnc/B/gx8GfhIv8MfNTdzweeyZh+N3C7u19AMMbLgXB6K3ArwbMhTie4m1okEnk7ZIPILL0VuAj4ebgzXgYcJhi2+evhPP8C3B+OX1/r7j8Op38F+KaZVQGr3f1bAO4+DBCu72fu3h7+vg1YAzye+z9L5FgKfpGAAV9x949NmWj2p9Pmm+sYJyMZ71Po355ESF09IoEfAr9hZssAzGyJmZ1G8G/kN8J5fhN43N17gC4ze2M4/Qbgx+HTzdrN7JpwHSVmVj6vf4XIDGivQwRw9+1m9ifAD8wsBowCNxE8lOS14WeHCc4DAPwW8IUw2DNHs7wB+KKZ/a9wHdfO458hMiManVPkJMys390ro65DJJvU1SMiUmC0xy8iUmC0xy8iUmAU/CIiBUbBLyJSYBT8IiIFRsEvIlJg/j+7ezhzX04w6QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9xlLIS51sNQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "c613eee6-2f93-4cd1-a1bf-07ded9f3ade3"
      },
      "source": [
        "#Visualize and predict outputs on few validation images\n",
        "act_model.load_weights('best_model.hdf5')\n",
        "prediction = act_model.predict(valid_img[:5])\n",
        " \n",
        "# use CTC decoder\n",
        "out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
        "                               greedy=True)[0][0])\n",
        " \n",
        "\n",
        "# see the results\n",
        "i = 0\n",
        "for x in out:\n",
        "    print(\"original_text  =\", valid_orig_txt[i])\n",
        "    print(\"predicted text = \", end = '')\n",
        "    for p in x:  \n",
        "        if int(p) != -1:\n",
        "            print(char_list[int(p)], end = '')       \n",
        "    print('\\n')\n",
        "    i+=1"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original_text  = 8B81013\n",
            "predicted text = 8B81013\n",
            "\n",
            "original_text  = 3B38140\n",
            "predicted text = 3B38140\n",
            "\n",
            "original_text  = 5B26675\n",
            "predicted text = 5B26675\n",
            "\n",
            "original_text  = 8T52099\n",
            "predicted text = 8T52099\n",
            "\n",
            "original_text  = 3AJ8930\n",
            "predicted text = 3AJ8930\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAkqZCIH16HG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "57724717-89eb-465f-aa7b-09c58dba7bd6"
      },
      "source": [
        "### Calculate accuracy on validation(test) set using the best saved model\n",
        "# predict outputs on few validation images\n",
        "prediction = act_model.predict(valid_img)\n",
        " \n",
        "# use CTC decoder\n",
        "out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
        "                         greedy=True)[0][0])\n",
        "\n",
        "##Predictions\n",
        "preds = []\n",
        "for p in out:\n",
        "  preds.append(labels_to_text(p))\n",
        "\n",
        "##Ground truths\n",
        "orig = []\n",
        "for i in range(len(valid_orig_txt)):\n",
        "  orig.append(valid_orig_txt[i])\n",
        "\n",
        "##Test Accuracy\n",
        "print('Test accuracy for the OCR solution is ',accuracy_score(orig, preds)*100,'%')"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy for the OCR solution is  96.18320610687023 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lHkFwJHbtGQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2f26f7f5-fb88-457f-b8bb-b193dfc69c77"
      },
      "source": [
        "### Test our OCR model on your own cropped license plate image\n",
        "def test_ocr(image_path):\n",
        "  # read input image and convert into gray scale image\n",
        "    img = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2GRAY) \n",
        "    img = cv2.resize(img, (128, 32))\n",
        "    img = np.expand_dims(img , axis = 2)\n",
        "    # Normalize each image\n",
        "    img = img/255.\n",
        "    img = np.expand_dims(img , axis = 0)\n",
        "    \n",
        "    ##Load our best saved model as act_model\n",
        "    act_model.load_weights('best_model.hdf5')\n",
        "\n",
        "    ##Predict using the trained model\n",
        "    prediction = act_model.predict(img)\n",
        "    # use CTC decoder\n",
        "    out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
        "                         greedy=True)[0][0])\n",
        "    # see the results\n",
        "    for x in out:\n",
        "        print(\"predicted text = \", end = '')\n",
        "        for p in x:  \n",
        "            if int(p) != -1:\n",
        "                print(char_list[int(p)], end = '')  \n",
        "\n",
        "## Change the image_path variable to your own image path \n",
        "image_path = 'test_image.png'  \n",
        "test_ocr(image_path)"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted text = 7B11123"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}